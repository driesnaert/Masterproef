\documentclass{report}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{bm}
\usepackage{amssymb}


\DeclareMathOperator*{\argmax}{argmax}


\theoremstyle{definition}
\newtheorem{exmp}{Example}[section]
\usepackage{tikz}
\usetikzlibrary{shapes,backgrounds}
\title{Titel}
\author{Dries Naert}
\date{\today}

\begin{document}

\maketitle
\tableofcontents
\newpage

\begin{abstract}
\end{abstract}

\section*{Introduction}

\chapter{Precise probabilities}
	\section{Sets}
	\section{Measures and probabilities}
	\section{Bayesian viewpoint}
	\section{Bayes' rule}
	In this section I will explain Bayes' rule, since this rule is the working mechanism of the Bayes classifier we'll construct later on. A simple way of explaining this rule is by letting an event correspond to a 2-dimensional geometric shape. The value of the probability of this event is then the area of the corresponding shape. The rule is here explained with 2 circular shapes. In general, this kind of setup is called an Euler diagram and a picture of the situation is drawn in \ref{figure:eulerfig}. Now suppose we see the picture and after that someone would assure us that event A actually has happened, some cutting and rescaling of the picture has to be done to fit it to the new information. \ref{figure:eulerfig2} is our new state of knowledge. Everything outside A has been cut off and the picture was rescaled so that $P'(A)=1$. This can be done by dividing all areas by the original area of A. The conditional probability of event B, given event A is its new area:
	
	\begin{equation}
	P(B|A) = \frac{P(A \cap B)}{P(A)} \text{ .}
	\end{equation}
	This is the rule of Bayes. It gives a way of calculating the probability of event B, given event A (happened).
	As a result of Bayes' rule, we also have a way of calculating $P(A \cap B)$, assuming we have information about $P(B|A)$ and $P(A)$. Calculating $P(B \cap A)$ this way is called the multiplication rule.\\
	
	\textbf{Example. } A man goes to a doctor and will undergo a test for a horrible disease. The result of this test can be positive (+) or negative (-). The probabilities of the test results, conditioned on the state of (not) being sick is given in table \ref{tabel:examplebayes}. The doctor tells the man that based on what he knows at that very moment, the probability of the man having the disease is $0.01$. If the test would be positive, what is the probability of the man having the disease? We can solve this by directly applying Bayes' rule, this yields
	
	\begin{equation*}
		\begin{split}
		P(Sick|+)&=\frac{P(+\cap Sick)}{P(+)}=\frac{P(Sick)P(+|Sick)}{P(+)}\\
		&=\frac{P(Sick)P(+|Sick)}{P(+|Sick)P(Sick)+P(+|\overline{Sick})P(\overline{Sick})}\\
		&= \frac{0.01\cdot 0.9}{0.9\cdot0.01 + 0.15\cdot0.99}\approx0.057 \text{ .}
		\end{split}
	\end{equation*}
	As you can see from the example, to apply Bayes' rule in practice, I used the multiplication rule to find numerator and both the law of total probability and the multiplication rule to find the denominator. Being sick or not being sick is indeed a partition of the whole probability space and this partitioning allowed me to use the law of total probability.  
	
	\begin{table}
		\caption{Test result probabilities}
		\label{tabel:examplebayes}
		\begin{center}
		\begin{tabular}{ |c|c|c| } 
		 \hline
	 	   & Sick & Not sick \\ 
	 	 \hline
	 	+ & 0.9 & 0.15 \\ 
	 	- & 0.1 & 0.85 \\ 
	 	\hline
		\end{tabular}
		\end{center}
	\end{table} 
	
		\section{Decision theory?}
	
	\chapter{Classification} \label{chap:classification}
The idea behind classification is that there is an unknown probability distribution $P(X_1,...,X_n,Y)$, from which we have drawn samples/data points $d(x_1,...,x_n,y)\equiv d(\bm{x},y)$. We'll call the $x$'s the features/input and $y$ the class/output. In the future we will be given specific feature values, and we want to predict what the class is. A classifier does exactly that. It is a function that maps features on classes. The class variable always takes discrete values, the features are allowed to be both discrete or continue. An example of such a distribution-features-class triplet could be a situation where the class is a person's favorite movie. The features could then be things like \lq likes this actor', \lq likes that actress', \lq likes westerns', etc. The distribution itself does not really exist, but it is represented by all people that have a favorite movie. We'll call such a representation of the distribution the population. It is typical in classification problems that the different classes are given. For the features you will often be able to choose whether to include a feature in the problem or not. For example, if you were with Netflix, you cannot go around the fact that you are interested in the movies your clients like. But it is up to you then to decide if features like gender or age of the client play a role in this problem. 
		
	\section{The hypothesis set}
Setting up the domain and codomain is one part of the initialization phase. Another part is determining what the hypothesis set $\mathcal{H}$ is. This hypothesis set is not anything more than a set of functions, also called hypotheses in this context,  with an appropriate domain and codomain. When everything is done, we will be given one final function/hypothesis $g$ that we will use to make future predictions. The idea of the hypothesis set is that the final hypothesis $g$ must be in our hypothesis set $\mathcal{H}$.
\paragraph{Example: linear functions.}
Take the specific case where we have 2 continuous features $x$ and $y$ which we will use to predict a binary class $c \in \{-1,1\}$. Note that the domain here is $\Re^2$, ie the plane. A possible hypothesis set for this problem could be
\begin{equation}
h:= (x,y) \rightarrow \text{sign}(w_0 + w_1 x + w_2 y),
\end{equation}
where $(w_0,w_1,w_2) \in \Re^3$, and

\begin{alignat*}{2}
\text{sign}(z) &= 1 &&\text{\qquad if } z \geq 0\\
& = -1 &&\text{\qquad else. }
\end{alignat*}
This hypothesis set corresponds to all lines in the plane. Points at one side of a line are mapped to one class while points at the other side are mapped to the other class. 
	
	\section{Cost and utility function}
A short recap of the situation. We are given a list of feature values $\bm{x}$ and will be asked to predict the class $y$. Giving a correct prediction of one class is a good thing, and giving a correct prediction of another class is also a good thing. However these 2 good things are not necessarily equally good. The same goes up for wrong predictions, 2 different wrong predictions are not necessarily equally avoidable either. The utility function is a way of putting numbers on how pursuable good is and how avoidable bad is. More precise, in the context of classifiers, a utility function $U(\hat{C},C)$, accepts 2 values on the same domain. The 1st variable is what the classifier predicts, the 2nd variable is what the class really is. The output of this utility function is a scalar that can be interpreted as some reward-like variable we'd like to be high in future events.\\
Utility functions can often be given in a table form. An example is given in table (\ref{tableclassification}). The classifier will have to predict whether tomorrow will be rainy or sunny. Correct predictions are treated equally good and are given a reward of 1. Predicting rain when there really will be sun is a little bit annoying, so it gets a punishment of -1. However, that wouldn't be as bad as predicting a sunny day when it really will be rainy so this one gets a punishment of -2.
\paragraph{Expected utility of a single hypothesis.} We can now define the expected utility $E_{\tiny{U}}$ of a single hypothesis $h$,
\begin{equation}
E_{U}(h)=\lim_{N \to \infty} \frac{1}{N} \sum_{i=1}^{N}U(h(\bm{x}_i),y_i),
\end{equation} 
where $\bm{x}_i$ and $y_i$ are drawn from an unknown distribution $P(X,Y)$. In words, this expected utility is the value we would obtain if we were to run a single hypothesis $h$ on the entire (infinite) population and calculate our average utility with this hypothesis. In other words, it is how much utility you will get on average if you use this hypothesis as classifier.
	
	\section{Generalization and bias-variance}
	\paragraph{Generalization}
If you have the expected utility of every hypothesis in your set, solving the problem becomes trivial. You then just pick the hypothesis with highest expected utility. However, we never have the real expected utility, what we do have is a dataset $\mathcal{D}$. We can calculate the average utility for every hypothesis on this dataset. We will name the average utility on the dataset $\hat{E}_{\tiny U}(h)$. The hat indicates that it is not a real expected value but an estimate based on the dataset. The generalization error $e$ is then defined, with $g$ the final hypothesis
\begin{equation} \label{generror}
e = \hat{E}_{\tiny U}(g) - E_{\tiny U}(g).
\end{equation}
When we come up with a final hypothesis, we want some guarantees that the generalization error is small. Something we can do to have a lower generalization error is to gather more data. Another thing is working with smaller hypothesis sets.\\
If we rewrite eq (\ref{generror}), we obtain
\begin{equation} 
E_{\tiny U}(g) = \hat{E}_{\tiny U}(g) - e. 
\end{equation}
This means that our real performance is the difference of the performance on the dataset minus the generalization error. We want the generalization error $e$ to be small and at the same time the $\hat{E}$ term to be large. This is a trade off. A large hypothesis set allows good performance on the training set (large $\hat{E}_{\tiny U}(g)$) but impairs the ability to generalize (large $e$ as well). On the other hand, small hypothesis sets have a poor performance on the training set but generalize better.
\paragraph{Bias and variance}
I will call the target function $f_t$ the function that satisfies
\begin{equation}
E_{U}(f_t) \geq E_{U}(g),
\end{equation}
where $g$ is just any function with the correct domain and codomain. The target function is the best function we can have for a given set of features. When this function is not in the hypothesis set we say that we have bias. If you now call $h_{\text{best}}$ a hypothesis that satisfies
\begin{equation}
E_{U}(h_{\text{best}}) \geq E_{U}(h),
\end{equation}
where here $h$ is any other function in the hypothesis set. This is the best function inside the hypothesis set. The amount of bias $b$ can then be defined as
\begin{equation}
b = E_{U}(f_t) - E_{U}(h_{\text{best}}).
\end{equation}
We will refine this definition a bit in the next section, because the bias has some dependence on the training algorithm as well. To explain what the variance is, think of the hypothesis set as being a darts board. The bulls eye corresponds to the best hypothesis $h_{\text{best}}$. Throwing a dart is then the equivalent of coming up with a final hypothesis. Now suppose that on average you throw in the center, ie on average you have the best hypothesis of the set. But that does not mean you are doing anything good. Fig (\ref{dartsvariantie}) shows the situation why not. Both throw on average to the center, but you'd be happier with the first one in your team.
We can intuitively decompose the average utility of a the final hypothesis $U_{\text{av}}$ on a way like
\begin{equation}
U_{\text{av}} = U_{\text{max}} - U_{\text{bias}} - U_{\text{var}}.
\end{equation}
This puts the trade off in a different perspective. Training on more data leads to a lower $U_{\text{var}}$ but leaves $U_{\text{bias}}$ unaffected. Working with larger hypothesis sets will in general increase variance and reduce bias, but this depends on the training algorithm as well.  
	\section{Training algorithm}
The training algorithm specifies how we will pick a hypothesis from the set. I illustrate how the same hypothesis set can have 2 different training algorithms. The goal will be to differentiate between 2 classes, indicated in the figures with crosses and circles. We will make the assumption that every dataset is always linear separable. This means that we will always be able to draw at least 1 line that separates the crosses and circles.
\paragraph{title} 

	\section{Cross-Validation}

\chapter{Naive Bayes classifier} 
A Naive Bayes classifier is, like all classifiers, a function. This classifier works in $2$ steps. First it maps its input onto a set of probabilities, $1$ for each of what we will call the classes (of the object). In a 2nd step, the classifier will use a decision rule to pick one of the classes, based on the previously calculated probabilities. A popular decision rule is to pick the class that has the highest probability, aimed at maximizing the number of correct classifications. One of the things we will do with this classifier, is to play with different decision rules. We'll also introduce indecisiveness, that is we allow the classifier to say  \lq I don't know what class this object belongs to'. The classifier will eventually have to estimate some parameters, which will end up not being different from certain probabilities.

\section{Bayes classifier}
If you would construct a Bayes classifier yourself, one of the things you will have to know is $P(C|F_1,F_2,...,F_n)$. Read this as the probability that C is in a certain class, given a combination of some features, the $F_i$'s, occurred. Comma's between the $F_i$'s have the same meaning as the intersection operator \lq $\cap$' in this context. A Bayes classifier will calculate this conditional probability by using, surprisingly, Bayes' rule. After applying the multiplication rule on the numerator, the formula becomes
\begin{equation} \label{eq:bayesclassifier}
P(C|F_1,F_2,...,F_n)=\frac{P(C)P(F_1,F_2,...,F_n|C)}{P(F_1,F_2,...,F_n)} \text{ .}
\end{equation}
The denominator will always be calculated by using the law of total probability, conditioning on the class. Now let's take a look at the expression $P(F_1,F_2,...,F_n|C)$. This is a function with a discrete domain $D=\{C \times F_1 \times F_2 \times ... \times F_n\}$ and continuous codomain $Co=[0,1]$. In general, machine learning methods do not try to construct a look-up table of input-output relations. Instead, typical methods parametrize an unknown function eg with radial basic functions (see also chapter \ref{chap:classification} for more information about this). We will do something similar here. But before simplifying things, let's manually count how many parameters we'd currently have to estimate. Note that these parameters I am talking about are not different from the probabilities of expression (\ref{eq:bayesclassifier}). Say the class we are interested in has $n_c$ possible values and we have $N$ features, with each feature having $n_i$ possible values. The denominator of (\ref{eq:bayesclassifier}) can be calculated if we can calculate the nominator, via the law of total probability, so let's not spend a parameter on the denominator. $P(C)$ is completely determined by $n_c-1$ parameters. To determine $P(F_1,F_2,...,F_n|C)$ we have to estimate for every element in C, $n_1 \cdot ... \cdot n_N -1$ parameters. Add everything together and we end up with $n_c \cdot n_1 \cdot ... \cdot n_N -1$ parameters to estimate. It should be clear that this number easily blows up because of the multiplications. This number will be reduced by the assumption of conditional independence as explained in section \ref{sec:conditionalindependence}.
\section{Conditional Independence} \label{sec:conditionalindependence}
Conditional independence is the assumption that makes the Bayes classifier a Naive Bayes classifier. To calculate the Bayesian probability we had the troubling function $P(F_1,F_2,...,F_n|C)$. Now let's expand this thing by applying Bayes' rule first and then the multiplication rule multiple times.
\begin{equation*}
\begin{split}
P(F_1,F_2,...,F_n|C)&=\frac{P(C \cap F_1 \cap ... \cap F_n)}{P(C)}\\
&=\frac{P(F_1|F_2 \cap ... \cap F_n \cap C)P(F_2\cap ...\cap F_n \cap C)}{P(C)}\\
&=\frac{P(F_1|F_2 \cap ...\cap F_n \cap C)P(F_2|F_3 \cap ...\cap F_n \cap C)P(F_3 \cap ... \cap F_n \cap C)}{P(C)}\\
&=...\\
&=\prod_{i=1}^{n}{P(F_i|\bigcap_{i+1}^{n}{F_i}\cap C)}
\end{split}
\end{equation*}
The defining assumption that we will call the conditional independence assumption is
\begin{equation}
P(F_j|\bigcap_{i \not= j}^{}{F_i}\cap C)=P(F_j|C)\text{,}
\end{equation}
where the intersection can be taken over any subset of $F_i$'s. This way equation (\ref{eq:bayesclassifier}) reduces to
\begin{equation} \label{eq:genbayes}
P(C|F_1,...,F_n)=\frac{\prod\limits_{i}^{}P(F_i|C)}{\sum\limits_{j}^{}\prod\limits_{i}^{}P(F_i|C_j)} \text{ .}
\end{equation}
This is the general form of the Naive Bayes classifier and will be used thoroughly. The numbers of parameters to estimate is reduced. Take the same example where we had $N$ features, with each feature $n_i$ possible values and $n_c$ classes. The total number of parameters V to estimate then becomes 
\begin{equation}
V=n_c(n_1 + ... + n_N - N)+n_c-1.
\end{equation}
The $-N$ in the sum arise from the N relations where $\sum P(F_i|C)=1$. Whereas in the case without the independence assumption we had as general formula for number of parameters $V'$,
\begin{equation}
V'=n_c(n_1 \cdot ... \cdot n_N)-1.
\end{equation}
\section{Examples} 
In this section I will show some fictitious examples to get some better understanding of the whole mechanism, when it can fail and when it should not fail. All features and classes in the examples will be binary unless stated otherwise, a binary feature I will call an attribute.\bigskip

\textbf{Example 1.} This example illustrates how 2 uninformative attributes are not so uninformative after all because the have perfect complementary information. In the setup we have 2 attributes $A$ and $B$ and a class $C$ such that $$P(C)=P(A|C)=P(A|\overline{C})=P(B|C)=P(B|\overline{C})=\frac{1}{2} $$ 
\begin{equation*}
\begin{split}
& A \text{ and } B \Rightarrow C \\
& \overline{A} \textrm{ and } \overline{B} \Rightarrow C \\
& A \textrm{ xor } B \Rightarrow \overline{C}.
\end{split}
\end{equation*}
In this situation the naive classifier will always tell us that $P(C|A,B)=\frac{1}{2}$. If instead the classifier would be allowed to drop the conditional independences, the function becomes deterministic and we can obtain a perfect accuracy. In situations like these we should join the two attributes to one feature that takes 4 values, 1 for every combination of the attributes. \bigskip

\textbf{Example 2.} This example is somehow the adverse of the previous. We begin with 2 truly conditional independent attributes A and B. Let
\begin{equation*}
\begin{split}
P(A|C)=P(B|C)=P(\overline{A}|\overline{C})=P(\overline{B}|\overline{C})&=0.8\\
P(\overline{A}|C)=P(\overline{B}|C)=P(A|\overline{C})=P(B|\overline{C})&=0.2\\
P(C)&=\frac{1}{2}\text{ .}
\end{split}
\end{equation*} 
We calculate the conditional probabilities of C, 

\begin{equation*}
\begin{split}
& P(C|A,B)=\frac{0.5 \cdot 0.8 \cdot 0.8}{0.5 \cdot 0.8 \cdot 0.8 + 0.5 \cdot 0.2 \cdot 0.2 }= 0.941 \\
& P(C|A,\overline{B})=P(C|\overline{A},B)=0.5 \text{ (because of the symmetry) }\\
& P(C|\overline{A},\overline{B})=1- P(C|A,B)=0.059 \text{ (because of the symmetry).}
\end{split}
\end{equation*}
Now we want to do better than that. I propose we add an attribute $A'$ to the classifier. The problem with this attribute $A'$ is that it is perfectly correlated with attribute A. Because the relation $(A \Leftrightarrow A')$, this one does actually not learn us anything at all and the real probabilities do not change. However, the estimated ones will.
\begin{equation*}
\begin{split}
& P(C|A,B,A')=\frac{0.5 \cdot 0.8 \cdot 0.8 \cdot 0.8}{0.5 \cdot 0.8 \cdot 0.8 \cdot 0.8 + 0.5 \cdot 0.2 \cdot 0.2 \cdot 0.2 } = 0.985\\
& P(C|A,\overline{B},A')=\frac{0.5 \cdot 0.8 \cdot 0.2 \cdot 0.8}{0.5 \cdot 0.8 \cdot 0.2 \cdot 0.8 + 0.5 \cdot 0.2 \cdot 0.8 \cdot 0.2}=0.8\\
& P(C|\overline{A},\overline{B},\overline{A'}) = 1-P(C|A,B,A') = 0.015 \text{ (because of the symmetry).}
\end{split}
\end{equation*}
If we compare the probabilities we'd like the classifier to give us and the ones with the extra, but very dependent, feature, we see the mechanism of how dependent features can bias probabilities.

\section{Decision rules}
For the time being, we assume perfect knowledge about $P(C|F)$ and focus our attention on how to decide. I'll show 3 decision rules, where the second and third rule will appear to be equivalent generalizations of the first one. To avoid confusion, class decisions made by the classifier will be accompanied with the hat symbol, so read $\hat C$ like \lq output of the classifier is $\hat C$'.  

\paragraph*{Maximize total accuracy}
A first way of picking a class $C$, is to pick the one for which $P(C|F)$ is highest. Symbolically, this rule gives
\begin{equation} \label{maxtotalaccuracy}
\hat{C}=\argmax_C P(C|F).
\end{equation}
I will show informal why this rule gives the highest number of classifications in the long run, also called total accuracy. Remember that for a frequentist there is no difference between $P(C)$ and $\lim\limits_{N \rightarrow \infty}\frac{N_C}{N}$, where $N$ is the total amount of cases presented and $N_C$ is the amount of times the object turns out to be in class $C$. This interpretation allows us to calculate the total accuracy,
\begin{equation} \label{maxacc}
TotAcc = \sum_{i}^{} \max_C P(C|F_i) P(F_i).
\end{equation}
The \lq max' comes from the decision rule. Given the probabilities $P(C|F_i)$, we pick the class with the highest probability. The value of $\max_C P(C|F_i)$ is then the accuracy we would obtain if only the $F_i$ feature combination would occur. What I do then, is to take a weighted sum of these partial accuracies over all possible feature combinations that can occur. The weights, which are the $P(F_i)$'s, represent the amount of times we will have these corresponding partial accuracies. If we now look at equation (\ref{maxacc}), the only way the decision rule affected the total accuracy is when we took the maximum of $P(C|F)$. We had no saying in the $P(F_i)$'s, the world decides these values for us. Now we see that no other rule than picking the class with highest probability can give a higher total accuracy.

\paragraph*{Threshold values}
We generalize the maximize total accuracy decision rule (\ref{maxtotalaccuracy}) with threshold values as follows
\begin{align} \label{tresholds}
\hat{C}&=\argmax_{C \in a} P(C|F)\text{, where}\\
a &= \{C \text{ for which } P(C|F)>t_C \} 
\end{align}
In words this means that if $P(C|F)$ is larger than some threshold $t_C$, the classifier puts C in a set of candidates. Note that $t_C$ may be different per class. Then it picks, as before, the class with highest probability in this set of candidates. An alternative to this would be to not choose when there is more than 1 candidate, however this does not generalize the first decision rule very well. The situation might occur that none of the probabilities passes its threshold and the set of candidates is empty. We call the classifier undecided in these situations. With decision rules that allow indecisiveness, we must clearly distinguish between the accuracy and total accuracy. In the following, let $Acc$ be the accuracy, $TotAcc$ the total accuracy, $d$ stands for decisiveness, $N_+$ is the number of correct classifications, $N$ is the number of cases presented and finally $N_d$ is the number of cases that were decided. Then we have the relations
\begin{equation} \label{tresholdnotations}
\begin{split}
Acc &= \frac{N_+}{N_d}\\
TotAcc &=\frac{N_+}{N}\\
d &= \frac{N_d}{N}.
\end{split}
\end{equation} 

\paragraph{Maximize expected utility}
In this context, the utility function $U({\hat{A}},C)$ should be read as what you will get as payoff (utility) when an agent (the classifier) decides to undertake action $\hat{A}$ and the class ends up being $C$. After we observed the feature combination $F$ and the probabilities $P(C|F)$ are calculated, we can calculate expected payoffs for different actions $\hat A$
\begin{equation}
E_{\mbox {\tiny U}}(\hat A|F)=\sum_{i}^{}U(\hat A,C_i)P(C_i|F).
\end{equation}
The rule here is to pick the action with highest expected value
\begin{equation}
\hat A = \argmax_{\hat A}E_{\mbox {\tiny U}}(\hat A|F).
\end{equation}
I'd like to stress that although $C$ can only take a number of values equal to the number of classes, $\hat A$ can take any number of values with general utility functions. This is because $\hat A$ does not represent a class with utility functions but an action that someone undertakes. Here, we will only work with utility functions where $\hat A$ can take the same values as $C$ plus 1 extra value, corresponding to the no decision made situation. The action in the utility function is then equivalent to the classifier (not) choosing a class. This way $\hat A = \hat C$, where $\hat C$ is still the classifiers output.

\section{Training the classifier}
So far, I have ignored the way we obtain the probabilities $P(C)$ and $P(F|C)$. In this section we look at some possibilities for getting these values.
\paragraph{Guessing}
 If someone has no data available but happens to be an \lq expert' in the matter, he can try to guess the probabilities. Whether this is a good idea might depend on the \lq expert'. I included this method just to show that the next one is not unique.
\paragraph{Use data}
In normal situations we have a set of input-output/feature-class pairs. To estimate all probabilities $P(F|C)$, we will give every feature its own table, I'll call these the feature tables. The number of rows in each one of these is then equal to the number of values that particular feature can have. The number of columns are the same for every table and are equal to the number of classes. Now we have to fill in some numbers in the tables. We do this by going over the given feature-class pairs one by one. One of these pairs looks like $(f_1,f_2,...,c)$. Then, we go for every feature to its table and add 1 to the entry of which the row and column correspond to the one's of the feature-class pair we were considering. Table \ref{tabeltraining} is an example of how one such feature table looks like after the numbers are filled in. Estimating the actual probabilities goes as follows. The probability $P(F_1 = f_1 | C = c)$ eg is estimated by looking in the table of feature $F_1$. In this table take the number of the row associated with $f_1$ and column associated with $c$ and divide this number by the sum of numbers in the column of $c$. The probabilities $P(C)$ can be estimated by looking in any one of the tables. $P(C=c)$ is estimated by taking the sum of the elements of the column corresponding to $c$ (in any table) and divide this number by the sum of all elements in the table. I illustrate this method by a simple example, see also table \ref{tabeltraining}. This would be the feature table for a second feature, which can take 3 values and were we have 3 classes. If we were interested in, let's say, $P(F_2=b|C_3)$ the calculation would be
$$
P(F_2=b|C_3) = \frac{8}{7+8+3}.
$$
The probability of $P(C = C_2)$ would then become
$$
P(C = C_2) = \frac{1+2+4}{45},
$$
because the sum of all elements in the table is 45.
\begin{table} \label{tabeltraining}
\caption{Feature table}
\begin{center}
\begin{tabular}{c|c c c}
    	  & $C_1$ & $C_2$ & $C_3$\\  \hline
 $F_2$= a & 5     & 1 	  & 7 \\
 $F_2$= b & 6     & 2 	  & 8 \\
 $F_2$= c & 9     & 4 	  & 3 \\
 
\end{tabular}
\end{center}
\end{table} 
\newline
This simple illustration should make an nice property of the Bayes classifier clear. Namely, we can train it by reading a line of the dataset, do a single plus one addition in every one of our feature tables and repeat for all data available in the world. In the end, or at any time we want, we do only a small amount of summations and divisions to get the probabilities. This very simple training algorithm has the property that the time it takes to train is proportional with the size of the dataset.

\paragraph{Laplace smoothing}
Laplace smoothing means that we do not initialize our feature tables with all zero's but with a small number $\alpha$, typically around 1. The interpretation is that we start from a prior belief that all probabilities are uniformly distributed. When experimental results flow in, we adapt our belief more and more towards what we have observed and further away from the prior belief. Take the coin flip example why this is not so stupid. You flip a coin one time and it lands tails up. This wouldn't make you conclude that $P(\text{tails})$ is 1. 






\chapter{Imprecise probabilities}
This chapter introduces and motivates imprecise probabilities. We will do the motivation through a thought experiment. Let's say you are in the situation where you are given a coin and some time to test the coin. After a while of testing the coin, you will be offered a series of bets whether a coin flip with this coin will be heads or tails. If you had been flipping the coin for days, you would probably get away with a strategy where you bet large amounts of money. That is because after days of flipping that one coin, you have a very good idea about the probabilities of the next coin flip. You can then calculate the expected payoff and bet only if the payoff is positive and not care about anything else. Now suppose you only had 20 seconds to test the coin. You'd still have some idea about the probabilities of the next toss. However, you are not assured to get away with the same strategy in this situation as when you were given days to test the coin. The reason is that based on those 20 seconds of testing, you have some idea about the probabilities of the next flip, but you can't really pinpoint some exact, \lq precise' probability on it. This is why we introduce imprecise probabilities. Because it offers a way out of having to calculate expected payoffs based on probabilities we are not sure off.\\
We can look at imprecision from a different viewpoint. Say we are in the situation where no testing or whatsoever has been done about a repeating process. This means we have no reason to have any belief at all about the probabilities of this process. Some people might say that you have to express this state by assuming uniform probabilities and update the probabilities when experimental results come in. For the coin that would give initially $P(\text{heads})=P(\text{tails})=1/2$. You would like to do some betting against these kind of people, first you fix the coin such that $P(\text{head})=1$, then offer them bets that give a positive expected payoff assuming uniform distributions. Our imprecise alternative is to express a totally unknowing state by saying \lq the probabilities could be anything between 0 and 1', ie for the coin $P(\text{heads}) \in [0,1]$. When you start experimenting, you can shrink that interval, and if you experiment a bit more, you can shrink it a bit more. This is indeed opposed to the first situation, where you start with a only one estimation and review that estimation based on experiments. Downside of imprecision is that taking decisions like whether to accept or decline bets are not trivial anymore. That's one thing we'll have to fix.

	\section{Credal set}
We do not limit our belief about the outcome of an event to one probability mass function but to a whole set of mass functions. We'll call such a set of mass functions a credal set $\mathcal{C}$. You can interpret the idea of credal set loosely as \lq I suspect that the true probability mass function is in my credal set'.\\
We will limit the credal sets we work with to convex one's. This means that
\begin{equation}
\begin{split}
& \text{ if } P_1(X)  \text{ and } P_2(X) \in \mathcal{C}, \; \text{then} \\
& P_3(X) = \alpha P_1(X) + (1 - \alpha)P_2(X) \in \mathcal{C}, \; \alpha \in [0,1].
\end{split}
\end{equation}
Note that $P_3(X)$ satisfies the requirements to be a probability mass function, $P_3(X)>0 \text{ and } \sum_{x}^{}P_3(x) = 1$.\\
You can talk about the expected value or expected utility of a decision of one single element in a credal set but the expected utility of a credal does not exist.
  
	\section{Geometric interpretation}
We will represent probability mass functions as points that lie within a regular simplex. First we'll explain what a regular simplex is. A simplex is a generalization of triangles and tetrahedrons to higher, or lower, dimensions. Concretely, an n-simplex consists of $n+1$ outer points which are linearly independent. The simplex is then formally defined as the smallest subset of the n-dimensional euclidean space that contains the outer points and is convex. Less formally it is just the extension of triangles and tetrahedrons to higher dimensions. A property of an n-simplex is that its outer faces are $(n-1)$-simplexes. A regular simplex is then the generalization of a regular triangle/tetrahedron, where all edges must have the same length. 

\paragraph{Example: the triangle}
Take the triangle, which is a 3-simplex. It has $2+1=3$ outer points and if you would remove just a tiny bit of a triangle, it is either not convex anymore or you have removed one of the outer points. So it is the smallest subset of the plane that contains its corner points and is convex. The outer faces of the triangle are the line segments that connect the corner points, and a line segment is indeed a 2-simplex.

\paragraph{}
There's a theorem that states that for every point inside a regular simplex the sum of the distances to the outer surfaces is a constant. We can now normalize the coordinate system such that this sum of the distances is always 1. This we say that a point $p$ within a regular normalized n-simplex is can be viewed as a probability mass function for an event with n possible outcomes. Every outer face of the simplex then corresponds to one of the outcomes, and the probability of this outcome is the distance between the point and the outer surface. It is also the other way around, every possible mass function can be represented as a point within a regular simplex (no proof). A credal set has then a representation as a convex (geometrical meaning) set within a regular normalized simplex.

\paragraph{Example: event with three outcomes}
%tekening en uitleg    
	
	\section{Two types of uncertainty}
Working with credal sets leads to 2 reasons to be uncertain about the outcome of an event. The first reason is the same as with precise probabilities. If a coin has a probability of $1/2$ to land tails up, you are uncertain about the outcome, but you'd still know how to bet. If a coin has an imprecise probability between 0 and 1 to land tails up you are uncertain about the outcome as well, but you wouldn't know how to place bets this time. This illustrates these 2 uncertainties are somehow different. In the precise case your uncertainty is because the model you use to predict the outcome of events is limited in its predictive power. In the imprecise case, your have an additional uncertainty because you don't know the parameters (=probabilities) of your model. I will call the type of uncertainty associated with precise probabilities a \lq precise uncertainty'. And analog, I will call the additional uncertainty that comes with credal sets an \lq imprecise uncertainty'.


	\section{Decision theory}
\paragraph{}
Just as the case is with precise probabilities, we will use credal sets to make decisions. Because you can't talk about the expected value/payoff of a credal set, we introduce some new decision rules. The convention I will use is that the final \lq decision' will always be presented as a list $\mathcal{A}$ of admissible sub choices. The set of sub choices consists of all possible outcomes plus the additional choice of not deciding. If a sub choice is admissible, it is included in the final list and vice versa, if a choice has made it to the final list it is said to be admissible. The set $\mathcal{A}$ of all admissible choices must be interpreted as sub choices between which we couldn't make out which is the better one. If a case ends up with more than one admissible choice and/or the choice of not deciding is admissible, we say a case is undecided.
\paragraph{}
We will recognize the two types of uncertainty described above in our eventual decisions. For simplicity, take an event with 2 outcomes, A and B. We then have 3 sub choices: A, B and C. The sub choice C is the additional choice where we explicitly do not decide. If the admissible choices are $\mathcal{A} = \{A,B \}$, we don't decide. The reason is that we are not sure if either choice A or B is the best one to pick. It will turn out this is a kind of imprecise uncertainty. Now, if the admissible choices would have been $\mathcal{A}=\{C\}$, we don't decide because not deciding is somehow the best choice. The reason we choose not to decide here, is because not deciding gives us better payoffs, not because we can't choose between A or B. This will turn out to be a case of precise uncertainty. The admissible choices can also be of a form like $\mathcal{A}=\{A,C\}$. This means we are not sure whether to choose A or not to decide. In this case we have both an imprecise and precise uncertainty, the rule here should be to not decide. 


\paragraph{Interval Dominance}
Definitely the simplest decision rule of all. What we do is to search for every sub choice for a worst and a best case scenario. These worst and best case scenarios correspond to a (different) mass function in our credal set $\mathcal{C}$. The best case scenario for sub choice $x$, utility function $U$ inside credal set $\mathcal{C}$ will be written symbolically as $\overline{E_U^{\mathcal{C}}}(x)$, whereas the worst case is then written as $\underline{E_U^{\mathcal{C}}}(x)$. The formal definitions of these best and worst case scenarios are \begin{equation}
\begin{split}
\overline{E_U^{\mathcal{C}}}(x) &= \max_{f \in \mathcal{C}} E_U(x|f) \overset{short}{=} \overline{E}(x)\\
\underline{E_U^{\mathcal{C}}}(x) &= \min_{f \in \mathcal{C}} E_U(x|f) \overset{short}{=} \underline{E}(x),
\end{split}
\end{equation}
where $E_U(x|f)$ is the expected payoff for choice $x$, given mass probability $f$ and utility $U$. This way we can construct intervals for every choice, which are lower and upper bounds on the expected payoff for that choice. These intervals do not bound what we will get as payoff, only what we expect to get as a payoff. If $\underline{E}(A)>\overline{E}(B)$, we say that choice A interval dominates choice B. The rule to be an admissible choice using interval dominance is that a choice $x$ is admissible if it is not interval dominated by any other choice.



\paragraph{E-admissibility} 

\chapter{Imprecise Naive Bayes classifier}
	(long intro)
	\section{Training algorithm}

\chapter{Performance and comparison of precise and imprecise methods}
	(short intro)
	\section{Measures of performance}
	\section{Precise Naive Bayes}
	\section{Imprecise Naive Bayes}
	\section{Discussion}  %


\end{document}
